#!/usr/bin/env bash

source /etc/AIROCmAL/AIROCmAL_path
source $AIROCmALdir/AIROCmAL_env/bin/ipex-llm-init -g --device Arc
aiinaalpkg=Ollama

#export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
export OLLAMA_NUM_GPU=999
export OLLAMA_RUNNERS_DIR=$AIROCmALdir/AIROCmAL_env/lib/python3.11/site-packages/ollama/runners
export no_proxy=localhost,127.0.0.1
##export ZES_ENABLE_SYSMAN=1
#source /opt/intel/oneapi/setvars.sh
#export SYCL_CACHE_PERSISTENT=1
#!export PATH=/llm/ollama:$PATH
export OLLAMA_HOST=0.0.0.0
export OLLAMA_INTEL_GPU=true
export ONEAPI_DEVICE_SELECTOR=level_zero:0
export DEVICE=Arc
source /opt/intel/oneapi/setvars.sh

AIROCmAL_update_Ollama () {
  cd $AIROCmALdir/$aiinaalpkg
  git fetch --all
  git branch backup-master
  git reset --hard origin/master
  git pull
  rm ./libref-$aiinaalpkg*
  rm ./requirements_$aiinaalpkg*
  wget https://raw.githubusercontent.com/JT-Gresham/AIROCmAL/main/Ollama/libref-$aiinaalpkg
  wget https://raw.githubusercontent.com/JT-Gresham/AIROCmAL/main/Ollama/user_customize_$aiinaalpkg_example.sh
  wget https://raw.githubusercontent.com/JT-Gresham/AIROCmAL/main/Ollama/requirements_$aiinaalpkg.txt
  AIROCmAL update Torch
  pip install --upgrade -r requirements_$aiinaalpkg.txt
  AIROCmAL update Intel
  cd $AIROCmALdir/AIROCmAL_env/lib/python3.11/site-packages/ollama
#  if grep -Fxq "import intel_extension_for_pytorch as ipex" __init__.py
#    then
#      echo "Intel extensions found in __init__.py...skipping"
#    else
#      sed -i 's|from ollama._client import AsyncClient, Client|import torch\nimport intel_extension_for_pytorch as ipex\nimport intel_extension_for_tensorflow as itex\n\nfrom ollama._client import AsyncClient, Client\n|g' __init__.py
#  fi
  cd $AIROCmALdir/$aiinaalpkg
rm -Rf "$AIROCmALdir/$aiinaalpkg/models"
ln -sf "$AIROCmALdir/shared/LLMs/Ollama" "$AIROCmALdir/$aiinaalpkg/models"

cd $pdirectory
$AIROCmALdir/$aiinaalpkg/user_customize_$aiinaalpkg.sh
cd $AIROCmALdir/$aiinaalpkg
}

Ollama_daemon () {
while [ 1 ]
do
openwebuipid=$(pgrep -f "open-webui serve")
openwebuimem=$(RSSfull=$(cat /proc/$openwebuipid/status | grep RSS); echo $RSSfull | cut -d\  -f2)
if [[ $openwebuimem -ge "1751916" ]]
	then
		sleep 6
	else
		Ollama_exit
fi
done
}

Ollama_exit () {
    kill $(pgrep -f "ollama-bin serve")
    kill $(pgrep -f "open-webui serve")
}
